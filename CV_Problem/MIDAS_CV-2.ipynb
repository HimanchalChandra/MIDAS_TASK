{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Firstly, I am reading my train_image and train_label with the help of pickle and pandas, and then I am converting each of them\n",
    "into numpy array and then dividing my training data into training and validation set(6000 training data and 2000 validation),and\n",
    "then reshaping them into 4d matrics for inputing this data into our neural-network architecture'''\n",
    "train_x = pd.read_pickle(\"train_image.pkl\")\n",
    "\n",
    "train_y = pd.read_pickle(\"train_label.pkl\")\n",
    "\n",
    "X = np.array(train_x)\n",
    "y = np.array(train_y)\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "x_train = X[:6000]\n",
    "x_test   = X[6000:]\n",
    "y_train = y[:6000]\n",
    "y_test   = y[6000:]\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],28,28,1)\n",
    "x_test = x_test.reshape(x_test.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''I used batch-size of 10 and 10 epochs for our training data, then converted each data into float type and divided it by\n",
    "255 as it is the highest RGB value for a pixel, and then I converted our output from array of labeled data\n",
    "(from 0 to nb_classes-1) to one-hot vector'''\n",
    "input_shape = (28,28,1)\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "num_classes = 7\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Its time to create our CNN architecture, the one which I was using contains 2 convolutional layer, 1 Maxpooling layer, \n",
    "2 dense layer, 1 flatten layer and 2 dropouts''' \n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''I have categorical_crossentropy as my loss function and adadelta optimizer, and we are fitting the training as well as the\n",
    "validation data'''\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The score is evaluated on the basis of performance on validation data\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am saving my model and my model weights\n",
    "model_json = model.to_json()\n",
    "\n",
    "with open(\"model_num.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"model_num.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am reading the test data using pandas and pickle\n",
    "test_x = pd.read_pickle(\"test_image.pkl\")\n",
    "X = np.array(test_x)\n",
    "\n",
    "print(X.shape)\n",
    "x_test = X.reshape(X.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am loading my previous made CNN Model and its weights\n",
    "json_file = open('model_num.json', 'r')\n",
    "\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"model_num.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_model.save('model_num.hdf5')\n",
    "loaded_model=load_model('model_num.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' At last I am using the test data to predict the classes of the images in it and save it in a CSV file named\n",
    "himanchal_chandra.csv '''\n",
    "a = np.arange(2000)\n",
    "b = loaded_model.predict_classes(x_test)\n",
    "\n",
    "data1 = ['image_index','class']\n",
    "data2 = a.T\n",
    "data3 = b.T  \n",
    "\n",
    "list1 = []\n",
    "for index, i in enumerate(data2):\n",
    "\tlist2 = [i, data3[index]]\n",
    "\tlist1.append(list2)\n",
    "\n",
    "with open('himanchal_chandra.csv', 'w+', newline='') as f:\n",
    "\twr = csv.writer(f)\n",
    "\twr.writerow(data1)\n",
    "\twr.writerows(list1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
